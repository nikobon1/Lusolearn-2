import { useState, useRef, useCallback } from 'react';
import { transcribeAudio, comparePronunciation, PronunciationScore, TranscriptionResult } from '../services/speechRecognition';

interface UseSpeechRecordingResult {
    isRecording: boolean;
    isProcessing: boolean;
    error: string | null;
    result: PronunciationScore | null;
    transcription: TranscriptionResult | null;
    startRecording: () => Promise<void>;
    stopAndEvaluate: (expectedText: string) => Promise<PronunciationScore | null>;
    reset: () => void;
}

export function useSpeechRecording(): UseSpeechRecordingResult {
    const [isRecording, setIsRecording] = useState(false);
    const [isProcessing, setIsProcessing] = useState(false);
    const [error, setError] = useState<string | null>(null);
    const [result, setResult] = useState<PronunciationScore | null>(null);
    const [transcription, setTranscription] = useState<TranscriptionResult | null>(null);

    const mediaRecorderRef = useRef<MediaRecorder | null>(null);
    const chunksRef = useRef<Blob[]>([]);
    const streamRef = useRef<MediaStream | null>(null);

    const startRecording = useCallback(async () => {
        try {
            setError(null);
            setResult(null);
            setTranscription(null);
            chunksRef.current = [];

            console.log('[Speech] üé§ Requesting microphone access...');

            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    sampleRate: 48000
                }
            });

            streamRef.current = stream;

            const mediaRecorder = new MediaRecorder(stream, {
                mimeType: 'audio/webm;codecs=opus'
            });

            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 0) {
                    chunksRef.current.push(event.data);
                }
            };

            mediaRecorderRef.current = mediaRecorder;
            mediaRecorder.start(100); // Collect data every 100ms
            setIsRecording(true);

            console.log('[Speech] üî¥ Recording started');

        } catch (err: any) {
            console.error('[Speech] ‚ùå Microphone error:', err);
            if (err.name === 'NotAllowedError') {
                setError('–î–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É –∑–∞–ø—Ä–µ—â—ë–Ω. –†–∞–∑—Ä–µ—à–∏—Ç–µ –¥–æ—Å—Ç—É–ø –≤ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞—Ö –±—Ä–∞—É–∑–µ—Ä–∞.');
            } else if (err.name === 'NotFoundError') {
                setError('–ú–∏–∫—Ä–æ—Ñ–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–æ–¥–∫–ª—é—á–∏—Ç–µ –º–∏–∫—Ä–æ—Ñ–æ–Ω –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.');
            } else {
                setError('–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ—Å—Ç—É–ø–µ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É: ' + err.message);
            }
        }
    }, []);

    const stopAndEvaluate = useCallback(async (expectedText: string): Promise<PronunciationScore | null> => {
        return new Promise((resolve) => {
            if (!mediaRecorderRef.current || !isRecording) {
                resolve(null);
                return;
            }

            const mediaRecorder = mediaRecorderRef.current;

            mediaRecorder.onstop = async () => {
                console.log('[Speech] ‚èπÔ∏è Recording stopped, processing...');
                setIsRecording(false);
                setIsProcessing(true);

                // Stop all tracks
                if (streamRef.current) {
                    streamRef.current.getTracks().forEach(track => track.stop());
                    streamRef.current = null;
                }

                try {
                    const audioBlob = new Blob(chunksRef.current, { type: 'audio/webm;codecs=opus' });

                    if (audioBlob.size < 1000) {
                        setError('–ó–∞–ø–∏—Å—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å –≥—Ä–æ–º—á–µ.');
                        setIsProcessing(false);
                        resolve(null);
                        return;
                    }

                    const transcriptionResult = await transcribeAudio(audioBlob);
                    setTranscription(transcriptionResult);

                    if (!transcriptionResult.transcript) {
                        setError('–†–µ—á—å –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å —á—ë—Ç—á–µ.');
                        setIsProcessing(false);
                        resolve(null);
                        return;
                    }

                    const score = comparePronunciation(expectedText, transcriptionResult.transcript);
                    setResult(score);
                    setIsProcessing(false);
                    resolve(score);

                } catch (err: any) {
                    console.error('[Speech] ‚ùå Processing error:', err);
                    setError('–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: ' + err.message);
                    setIsProcessing(false);
                    resolve(null);
                }
            };

            mediaRecorder.stop();
        });
    }, [isRecording]);

    const reset = useCallback(() => {
        setError(null);
        setResult(null);
        setTranscription(null);
        setIsProcessing(false);
        setIsRecording(false);

        if (streamRef.current) {
            streamRef.current.getTracks().forEach(track => track.stop());
            streamRef.current = null;
        }
    }, []);

    return {
        isRecording,
        isProcessing,
        error,
        result,
        transcription,
        startRecording,
        stopAndEvaluate,
        reset
    };
}
